{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84713fc9-4a3f-4c84-9e75-39a27d1dfb06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3cnMul7t0WF",
    "outputId": "18f69166-25c2-46b3-ca90-c50894095c4d"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d816ea7-88cc-466c-a4f0-eefad92a7e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0A8f1dASuXfB",
    "outputId": "97484ecf-4be6-47b9-c43e-f431a1228cce"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "data = [(1,'krish') , (2,'ajay'),(3,'james')]\n",
    "columns = [ 'id' , 'name' ]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b630e910-19f3-4c49-8159-ee149528f1fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOKAP904u0Jr",
    "outputId": "223c2803-b896-4ec1-b9d8-955637fedc61"
   },
   "outputs": [],
   "source": [
    "#header = True: treat the first row of the csv as headers, if header = False then the first row will _c01, _c02\n",
    "#inferschema = True: understand the schema of hte table from csv if its false then treat all the data string.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "df = spark.read.csv('/content/sample_data/california_housing_test.csv',header=True,inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "#df.show()\n",
    "df.printSchema()\n",
    "#_c01 _c02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63590d56-ad54-48bf-bee8-7e75ac746510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-H-0ZztFblC4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73960084-2604-476a-b5d5-d94c589fd45c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWgjBPNCwtQ4",
    "outputId": "66761d65-256d-4bc8-f330-5bb7ceadadfe"
   },
   "outputs": [],
   "source": [
    "newdf = df.filter(df['longitude']<-123)\n",
    "newdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c83f1c4f-ec93-4b22-87cc-a1239afffc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swAQpkD0x_gA",
    "outputId": "c7109596-274c-4127-c1ef-f18760ac6210"
   },
   "outputs": [],
   "source": [
    "#withColumn: add a new column\n",
    "df= df.withColumn('sumdata',df['longitude']+df['latitude'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb947ed5-1e8d-4664-b855-cc030674315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mBEbPsN2yWpu",
    "outputId": "afdee709-b5a1-4f93-e6ec-4b9e2dd1018c"
   },
   "outputs": [],
   "source": [
    "#groupby : applied when we aggregating data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5469225-0659-449c-8569-6142e3de55d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "R0eE_9dpytPW"
   },
   "outputs": [],
   "source": [
    "ordersdf = spark.read.csv('/content/orders.csv', header  = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d50eae9-8dd1-4282-bf35-c041a0b24030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHDlXozYy6aV",
    "outputId": "315b9bfe-29b1-44a5-c06f-9044eeb269a1"
   },
   "outputs": [],
   "source": [
    "ordersdf.show()\n",
    "#type(ordersdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b5cd81-82b0-4dde-aaf2-607e649234cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RO-VDdABy8Wd"
   },
   "outputs": [],
   "source": [
    "customer_df = spark.read.csv('/content/customers.csv',header= True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c540658-0600-417d-bc3c-8fa0f2c2f28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nj3yJvl1_UKb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d93a9e02-2678-45f3-b1f9-22357c86fce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "h4xHGwR_zYTz",
    "outputId": "7942561a-c6cc-4d70-9348-a6df84674927"
   },
   "outputs": [],
   "source": [
    "#join dataframes\n",
    "mergeddf = ordersdf.join(customer_df,on='customerid', how = 'inner')\n",
    "newdf= mergeddf.select('CustomerID','Name','Quantity','Country')\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25e778e4-7b1d-425b-8568-bb55ff2f3e27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxQMk31Hzqtk",
    "outputId": "d31b733a-ff71-4e5f-c97f-436fee507cd1"
   },
   "outputs": [],
   "source": [
    "#sortdata\n",
    "newdf = mergeddf.orderBy('Quantity', ascending=True)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aadf9bd-7f37-4ddf-9bb0-abd4412a73f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "o4JbOXxx0Vdj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fd054e9-4ee9-4a01-b369-b458d784f326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2lZ9jNV0kzI",
    "outputId": "691c087a-107c-47f0-ee54-ddbdd3dc89ec"
   },
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "newdf = mergeddf.dropDuplicates(['customerid'])\n",
    "newdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1e93599-e218-41e5-be4a-db96fe089280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "MYgaPnge02Pi"
   },
   "outputs": [],
   "source": [
    "#udf: user defined function\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType,IntegerType, FloatType\n",
    "def to_uppercase(s):\n",
    "  return s.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b9f0df-c22b-4f8f-9ac3-e39a28d61758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "e8pP6k8q1nfk"
   },
   "outputs": [],
   "source": [
    "myudf = udf(to_uppercase, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4c8de12-ed7b-42d0-a2d8-c3ea37dd2035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "y1I_XLvb17pH",
    "outputId": "cbc02a7d-fc28-4ee1-f4c9-1c3b3e68fa89"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "\n",
    "def to_uppercase(s):\n",
    "  return s.upper()\n",
    "\n",
    "myudf = udf(to_uppercase, StringType())\n",
    "\n",
    "mergeddf = ordersdf.join(customer_df, on='customerid', how='inner')\n",
    "\n",
    "\n",
    "#download udf\n",
    "#create a udf object with output data type and function u need to apply\n",
    "#create a new column and pass the udf with the parameter as the columns u to apply it to.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73dc637c-66a4-4735-9189-90187aea33ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDJRJd_r2K5P",
    "outputId": "f0bfb831-8eca-4a47-e502-dfb12ded08b6"
   },
   "outputs": [],
   "source": [
    "mergeddf.show()\n",
    "mergeddf.withColumn('uppercase_name', myudf('Name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf401f26-7058-4f7a-95a5-a872fbc224fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xNkTb-aF2O_S"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def totalprice(quantity, price):\n",
    "  return quantity * price\n",
    "\n",
    "# Use FloatType if price or quantity might be floats\n",
    "myudf = udf(totalprice, FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ae0c6aa-6813-4d8c-8892-4c6492581e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "2FzViLvA3CJt",
    "outputId": "3281fe0d-603b-4435-b10f-bab195fc7e3d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def totalprice(quantity, price):\n",
    "    return quantity * price\n",
    "\n",
    "myudf = udf( totalprice , IntegerType())\n",
    "\n",
    "udf\n",
    "mergeddf = mergeddf.withColumn('total_price', myudf( mergeddf['quantity'], mergeddf['price'] ))\n",
    "\n",
    "mergeddf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2a38233-4360-4f97-8410-fde1f033c31d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLfSR8ei3gZG",
    "outputId": "a9dcb9af-362e-4b63-e1d0-303d586adfff"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, 'Alice', 'HR', 50000),\n",
    "    (2, 'Bob', 'HR', 60000),\n",
    "    (3, 'Charlie', 'Sales', 55000),\n",
    "    (4, 'David', 'Marketing', 45000),\n",
    "    (5, 'Eve', 'Finance', 70000),\n",
    "]\n",
    "\n",
    "columns = ['employee_id', 'employee_name', 'department', 'salary']\n",
    "\n",
    "employee_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "employee_df.show()\n",
    "\n",
    "\n",
    "#with cte as (select employeename, salary, department, dense_rank() over (parition by department order by salary desc) as ranking\n",
    "#from employees)\n",
    "\n",
    "#select * from cte where ranking =2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cc6d007-bc1a-4715-8455-7823b02771d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HWGyXJPd4XE2"
   },
   "outputs": [],
   "source": [
    "# cte\n",
    "# partition by department\n",
    "# order by salary desc\n",
    "# dense_rank() over (partition by department order by salary desc) as ranking\n",
    "\n",
    "\n",
    "select * from cte\n",
    "where ranking = 2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6093f920-36b7-4c11-a9e1-5813c86fb06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zaMazR5K5Qpn"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32cbd62a-6c7a-495d-8e99-1242b7bb3357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BwHDzh-75TVE"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d620345a-dd47-4218-b960-b73f2c909de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyDUGd6_5Cl9",
    "outputId": "b8748812-bf80-4623-e174-d31a6ecb4a9b"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank, lead,lag,rank,row_number, col\n",
    "\n",
    "window_spec  = Window.partitionBy(col('department')).orderBy(col('salary').desc())\n",
    "ranked_df = employee_df.withColumn('rank', dense_rank().over(window_spec))\n",
    "ranked_df.show()\n",
    "\n",
    "\n",
    "#window : pyspark.sql.window | pyspark.sql.functions | pyspark.sql.types import IntegerType, StringType, FloatType |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dba4ef3-9d5f-4761-b4a3-efdea4463536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YhsK_2FifTBU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b13447-9937-49c7-b0a7-1d0ab51c9eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpZLQkXx5fFG",
    "outputId": "89a6185d-329f-4e8c-ae5a-c4fd4a1a069a"
   },
   "outputs": [],
   "source": [
    "secondhighestsalary = ranked_df.filter( col('rank')==2 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40a261c9-caba-4422-9448-508e99e63a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxAdJdC7VZ_X",
    "outputId": "30405119-6d98-4272-c38a-a4bed835d85d"
   },
   "outputs": [],
   "source": [
    "#second highest salary by department\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col,dense_rank\n",
    "\n",
    "data = [\n",
    "    (1, 'Alice', 'HR', 50000),\n",
    "    (2, 'Bob', 'Engineering', 60000),\n",
    "    (3, 'Charlie', 'HR', 55000),\n",
    "    (4, 'David', 'HR', 45000),\n",
    "    (5, 'Eve', 'Engineering', 70000),\n",
    "]\n",
    "\n",
    "columns = ['employee_id', 'employee_name', 'department', 'salary']\n",
    "\n",
    "employee_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "windowspec = Window.partitionBy('department').orderBy(col('salary').desc())\n",
    "\n",
    "ranked_df = employee_df.withColumn('rank', dense_rank().over(windowspec))\n",
    "\n",
    "second_highest_df = ranked_df.filter( col('rank') == 2 )\n",
    "\n",
    "second_highest_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31189a49-9f5a-416b-9249-df53c7610908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymccmUiSXa4B",
    "outputId": "da3fb2ce-4e43-48e0-a6e4-7a0203208607"
   },
   "outputs": [],
   "source": [
    "employee_df.createOrReplaceTempView('employee')\n",
    "\n",
    "spark.sql('select * from employee where department= \"HR\"').show()\n",
    "spark.sql('select * from employee where department= \"IT\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c13b2e-c5f8-4207-af36-ffaea818eb8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tTWbWP4GDsXg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58e3e56f-6351-4095-a744-a838f6183744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_yXHrkcW6cU",
    "outputId": "c75682a1-3499-44d1-988e-903732f8f99b"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select department, avg(salary) as avgsalary from employee group by department\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a46ce31f-d482-42f1-8aeb-0711bf90b49a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cy1eTOg6YHXG",
    "outputId": "2a92dc14-5644-4f12-a0ff-0a994205943e"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from employee order by salary desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f899a3ba-c35b-40f4-937f-ef118bf4970c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mpvpxGjQbuRl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af30fe2f-8c63-402f-8f8a-671cf31b5094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kP-fSydq5e2a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adb8ca49-2f9f-4fd3-801d-1b2cccd1827d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WRyIJexYTyG",
    "outputId": "5021cf53-fb7c-4b3d-901a-c072a5b9c052"
   },
   "outputs": [],
   "source": [
    "query = \"\"\" select department, employee_name, salary from (select department, employee_name, salary, dense_rank()\n",
    "over (partition by department order by salary desc) as rank from employee) where rank = 2\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8af83c76-7ffe-4817-977e-9b5e2cc648b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0ACVH8tiYlxT"
   },
   "outputs": [],
   "source": [
    "#most frequently asked interview question : extract data from one source to another apply simple transformation\n",
    "# Data source format\n",
    "# Transformattion to be applied\n",
    "# use case : why are we doing this ?\n",
    "# final format of data?\n",
    "# frequency of data\n",
    "# size\n",
    "\n",
    "#SQL server : connections\n",
    "\n",
    "# define the source link: link = 'mylink/data.csv'\n",
    "# destinationpath = 'mypath/newdata.parquet'\n",
    "# extract: spark.read.csv(link)\n",
    "# df.drop_duplicates()\n",
    "# df.withColumn('city',lit('bangalore'))\n",
    "# df.write.parquet(destinationpath)\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "\n",
    "df = spark.read.csv('mylink/data.csv')\n",
    "df.dropDuplicates()\n",
    "dfwithnewco = df.withColumn('city', lit('bangalore'))\n",
    "dfwithnewco.show()\n",
    "\n",
    "# id name age city\n",
    "# 1 a 16 bangalore\n",
    "# 2 b 16 bangalore\n",
    "# 3 c 18 bangalore\n",
    "\n",
    "\n",
    "\n",
    "#scenario 2 : data is sitting in sql server\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "import mysqlconnector-python\n",
    "mysqlurl = 'myurl'\n",
    "\n",
    "# employeetable = 'employee'\n",
    "# departmenttable = 'department'\n",
    "mysqltable = ['employee','department']\n",
    "\n",
    "mysqlproperties =\n",
    "{\n",
    "    'host':'myhost',\n",
    "    'user':'root',\n",
    "    'password':'password',\n",
    "    'driver': 'mydriver'\n",
    "}\n",
    "\n",
    "\n",
    "#interview version\n",
    "\n",
    "mysqlurl = 'myurl'\n",
    "mysqltables= table1\n",
    "\n",
    "employeedf =   spark.read.jdbc(url=mysqlurl, table=mysqltable[0], properties=mysqlproperties)\n",
    "departmentdf = spark.read.jdbc(url=mysqlurl, table=mysqltable[1], properties=mysqlproperties)\n",
    "\n",
    "employeedf.show()\n",
    "departmentdf.show()\n",
    "\n",
    "#joined the dataframes\n",
    "mergeddf = employeedf.join(departmentdf, employeedf.departmentid == departmentdf.departmentid, how = 'inner')\n",
    "\n",
    "#change the name of the column : withColumnRenamed(col('name),'newname)\n",
    "\n",
    "#drop duplicates\n",
    "mergeddf.dropDuplicates(['employeeid']).show()\n",
    "df.withColumn('city', lit('bangalore'))\n",
    "mergeddf.write.parquet('path')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2458b60f-bfb7-43f7-8e90-d42060ddca4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "zlQPof-N93G5"
   },
   "outputs": [],
   "source": [
    "#we have data in a database , extract data (employee and department).\n",
    "\n",
    "sqlproperties= {\n",
    "url:myurl\n",
    "id:admin\n",
    "password:password\n",
    "host:localhost\n",
    "driver:com.mysql.cj.jdbc.Driver\n",
    "}\n",
    "\n",
    "tables = ['employee','department']\n",
    "\n",
    "employee_df = spark.read.jdbc(url = url, table = 'employee', properties = sqlproperties)\n",
    "department_df = spark.read.jdbc(url = url, table = 'department', properties = sqlproperties)\n",
    "\n",
    "#join hte data and tranform\n",
    "\n",
    "newdf  = employeedf.join(departmetndf, employeedf.departmentid == departmentdf.departmentid, how = 'inner')\n",
    "\n",
    "newdf.withColumn('city', lit('bangalore'))\n",
    "\n",
    "#write the data in parquet format in a location\n",
    "\n",
    "newdf.write.parquet(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487d1683-e4b2-4c4d-9fa0-8f44693ff7b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kKXGG2BkNsaj"
   },
   "outputs": [],
   "source": [
    "#read data from csv\n",
    "#apply udf\n",
    "#filter\n",
    "#join\n",
    "# window functions\n",
    "#create a new column\n",
    "#read data from json or csv\n",
    "#read data from databse  : jdbc connection  spark.read.jdbc\n",
    "#write data into parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb8487b4-fd9d-4d64-99b1-9240a37005d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7DCOxTJSeQUP"
   },
   "outputs": [],
   "source": [
    "!pip install mysql-connector-pythonc\n",
    "\n",
    "import mysql.connector #mysql database\n",
    "import psycopg2 #redshift | postgressql\n",
    "import sqlalchemy #anything generic\n",
    "\n",
    "\n",
    "config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': 'password',\n",
    "    'database': 'database'\n",
    "}\n",
    "\n",
    "connection = myurl\n",
    "cursor = connection.cursor()\n",
    "\n",
    "query = 'select * from employee'\n",
    "cursor.execute(query)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "for row in result:\n",
    "  print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b7dcad-0b0b-4b7d-92a9-61a75c05623e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Xk6hE-6se_Ql"
   },
   "outputs": [],
   "source": [
    "#internal SQL databases\n",
    "#1) email ingestion\n",
    "#2) dump the csv file into a folder any folder (SFTP -secure file transfer protocol)\n",
    "#3) API integration (advantage it can customised and secured, disadvantage: money to develop and manage)\n",
    "#4) JDBC - plug and play.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aea4cba4-9186-4724-8f13-a86ef2df223d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0eYckwV0gwAB"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "mysqlurl = 'jdbc:mysql://localhost:3306/database'\n",
    "mysqltable = 'employee'\n",
    "\n",
    "engine = create_engine(mysqlurl)\n",
    "\n",
    "df = pd.read_sql_table(mysqltable, engine)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5d552ba-6a89-4d55-892c-f44fd1ec63ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "h5ymQi4khrab"
   },
   "outputs": [],
   "source": [
    "#psyopg2 - postgres sql.\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "connection = {\n",
    "\n",
    "    host = 'localhost',\n",
    "    database =  'mydatabase'\n",
    "    user = 'myuser',\n",
    "    password = 'mypassword'\n",
    "    port='5432'\n",
    "}\n",
    "\n",
    "connection = psycopg2.connect(**connection)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "query = 'select * from employee'\n",
    "cursor.execute(query)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "for row in result:\n",
    "  print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "641e32dc-5b47-45f1-bdf3-f6315cc5b545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QiogczwX8_I7"
   },
   "outputs": [],
   "source": [
    "#exact Interview coding questions:\n",
    "\n",
    "#Initialise and import relevant libraries\n",
    "#how to import from database, s3bucket, datawarehouse (mysql, postgressql, s3bucket, jdbc connections)\n",
    "#tranformations: create a new column, filter data,select data,sort, drop duplicates, merge dataframes, apply window function for second highest salary, UDF\n",
    "#write data in csv format, parquet format or jdbc connection to a database (output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7e72e27-21a6-4032-9c85-c56a932c5daf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aVgRyfBVj4Sy"
   },
   "outputs": [],
   "source": [
    "##example real world with coalesce, repartition, cache, persist, broadcast joins.\n",
    "\n",
    "#ecommerce:\n",
    "#usecase: analyse customer behaviour to provide custom offers and deals depending on customer behaviour|\n",
    "#daily transactional dashboard (sales, returns, profits, losses, top selling item, lowest selling items\n",
    "#top selling location, least selling location, Top selling category, least selling category, male vs female shoppers )\n",
    "\n",
    "\n",
    "#Data sources\n",
    "\n",
    "#internal database:JDBC: mysql database (transaction, customer, product, logistics)\n",
    "#external data: API: api integration into google (trends , analytics), facebook (marketing spend, competitor marketing data) and 3rd party api's (benchmarking -competitor pricing ($1))\n",
    "#Internal / external : SFTP(secure file transfer protocol) Folder :  - csv data: from individual departments ( customer survey, marketing department) :pysftp\n",
    "\n",
    "#orders | customer | product|google analytics(external)\n",
    "\n",
    "#3 tables:\n",
    "\n",
    "#1) total orders by customer (orders, customer)\n",
    "#2) amount spent by customer (orders and customers)\n",
    "#3) date and time of the order by customer (orders)\n",
    "#4) product category(product, orders, customer)\n",
    "#5) location of customer (customer)\n",
    "#6) popular product based on location, brand and pricing.(product, orders, customer)\n",
    "#7) spend by gender,age, location, category.(orders, customer,product)\n",
    "\n",
    "#Data engineer goals:\n",
    "# I wrote scripts to extract the data from jdbc and sftp\n",
    "# once the data was extracted i was responsible for the below:\n",
    "\n",
    "# join the data set to get a consolidated view of purchased\n",
    "# apply tranformations to clean and filter the data\n",
    "# optimise the pipeline for large scale processing using coalesce, repartition, cache, persist and broadcast joins.\n",
    "\n",
    "\n",
    "#customer: customerid | name | city\n",
    "\n",
    "#orders : orderid | productid | quantity| orderdate|customerid\n",
    "\n",
    "#product: productid | name| category | price\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "customers_data = [(1, 'Alice', 'New York'), (2, 'Bob', 'San Diego'), (3, 'Charlie', 'Seattle')]\n",
    "orders_data = [(101, 1, 201, 2, '2023-12-25'), (102, 2, 202, 1, '2023-12-26'), (103, 3, 203, 3, '2023-12-27')]\n",
    "products_data = [(201, 'Laptop', 'Electronics', 1000), (202, 'Smartphone', 'Electronics', 700), (203, 'Headphones', 'Accessories', 150)]\n",
    "\n",
    "\n",
    "s3bucket = 'path/rawfolder/data.csv'\n",
    "accesskey = 'accesskey'\n",
    "secretkey = 'secretkey'\n",
    "\n",
    "#step 1: automatically extract data into your df\n",
    "\n",
    "customerdf = spark.read.csv('s3bucket',header = True, inferSchema=True)\n",
    "ordersdf = spark.read.csv('s3bucket',header = True, inferSchema=True)\n",
    "productsdf = spark.read.csv('s3bucket',header = True, inferSchema=True)\n",
    "\n",
    "#step2: connect using jdbc\n",
    "\n",
    "url = 'jdbc:mysql://localhost:3306/database'\n",
    "connectionproperties = {\n",
    "    'user':username,\n",
    "    'password':password,\n",
    "    'host' = 'host'\n",
    "    'driver':'com.mysql.cj.jdbc.Driver'\n",
    "}\n",
    "\n",
    "#option 1: read data directly from the database using jdbc\n",
    "customerdf = spark.read.jdbc(url='url', user= connectionproperties['username'], password=connectionproperties['password'], configuration= connectionproperties)\n",
    "ordersdf = spark.read.jdbc(url='url', user= connectionproperties['username'], password=connectionproperties['password'], configuration= connectionproperties)\n",
    "productsdf = spark.read.jdbc(url='url', user= connectionproperties['username'], password=connectionproperties['password']sword, configuration= connectionproperties)\n",
    "\n",
    "\n",
    "# option2 :manual dataframe creation\n",
    "customers_df = spark.createDataFrame(customers_data, ['customer_id', 'customer_name', 'city'])\n",
    "orders_df = spark.createDataFrame(orders_data, ['order_id', 'customer_id', 'product_id', 'quantity', 'order_date'])\n",
    "products_df = spark.createDataFrame(products_data, ['product_id', 'product_name', 'category', 'price'])\n",
    "\n",
    "customers_df.show()\n",
    "orders_df.show()\n",
    "products_df.show()\n",
    "\n",
    "\n",
    "#broadcast join - broadcast smallest dataframe to each node to reduce shuffling.\n",
    "products_broadcast = products_df.broadcast()\n",
    "\n",
    "#join the data\n",
    "\n",
    "order_details_df= orders_df.join(products_broadcast, on='product_id', how='inner')\n",
    "\n",
    "filtered_df = order_details_df.filter(col('quantity')>1)\n",
    "newdf = filtered_df.withColumn('total_price', col('quantity') * col('price'))\n",
    "\n",
    "#cache results for reuse\n",
    "newdf.cache()\n",
    "\n",
    "#aggregration\n",
    "\n",
    "aggregated_df = newdf.groupBy('city', 'category').sum('total_price')\n",
    "\n",
    "#repartition data for further processing\n",
    "\n",
    "repartition_df  = aggregated_df.repartition(4)\n",
    "\n",
    "#run the remaining aggregrations here\n",
    "\n",
    "#coalesce to optimise partitions for writing\n",
    "\n",
    "finaldf= repartition_df.coalesce(1)\n",
    "\n",
    "#write_output\n",
    "\n",
    "finaldf.write.mode('overwrite').parquet('/content/output')\n",
    "\n",
    "#1) total orders by customer\n",
    "#2) amount spent by customer\n",
    "#3) date and time of the order by customer\n",
    "#4) product category\n",
    "#5) location of customer\n",
    "#6) popular product based on location, brand and pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a73aa64f-e729-448d-8f1c-6244aa6d4ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sIJEE-KZfU3C"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "df = spark.read.parquet('file.parquet')\n",
    "\n",
    "dfwithnewco = df.withColumn('city', lit('bangalore'))\n",
    "\n",
    "dfwithnewco.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aad5cf42-1df9-4d63-90bf-9827fb8ebc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7bEw_Elf4xG",
    "outputId": "f9c2cbd4-9fb7-437b-f769-bc6c37ae5504"
   },
   "outputs": [],
   "source": [
    "#JDBC integration with mysql > s3 > redshift\n",
    "#API integration to 3rd party api\n",
    "#sftp integration\n",
    "#SQL: removing duplicates (row number, having, distinct)\n",
    "#pandas df and pyspark df\n",
    "#RDD vs df\n",
    "#joins in pyspark\n",
    "#broadcast variable / join\n",
    "\n",
    "#working for a ecommerce firm, we are getting data from multiple sources which includes sftp folder. the csv files from sftp are moved to s3 bucket, from s3 bucket extract the data into a sparf df\n",
    "#write tranformations  : remove duplicates, count of na in each column, fillna, create a new column to run some aggregrations.\n",
    "#i want to write the file to 2 places. one is redshift table and another is an s3 bucket. i want the file to be writter in parquet format.\n",
    "# I want to create a dataframe manually\n",
    "\n",
    "#initialise spark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, when, lit, count, sum, avg, max, min, countDistinct\n",
    "\n",
    "spark = SparkSession.builder.appName('practice').getOrCreate()\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('customer_name', StringType(), True),\n",
    "    StructField('city', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 'Alice', 'New York', 25, 'Female', 50000),\n",
    "    (2, 'Bob', 'San Diego', 30, 'Male', 60000),\n",
    "    (3, 'Charlie', 'Seattle', 35, 'Male', 55000),\n",
    "    (4, 'David', 'Los Angeles', 28, 'Male', 45000),\n",
    "    (5, 'Eve', 'Chicago', 32, 'Female', 70000),\n",
    "    (6, 'Frank', 'Miami', 29, 'Male', 55000),\n",
    "    (7, 'Grace', 'San Francisco', 31, 'Female', 65000),\n",
    "    (8, 'Henry', 'Boston', 27, 'Male', 52000)\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "#s3 credentials\n",
    "\n",
    "s3bucket = 'path/rawfolder/data.csv'\n",
    "accesskey = 'accesskey'\n",
    "secretkey = 'secretkey'\n",
    "\n",
    "#redshift credentials:\n",
    "\n",
    "redshift_table = 'tablename'\n",
    "redshift_cluster = 'clustername'\n",
    "redshift_url = 'jdbc:redshift://endpoint:5439/dbname'\n",
    "username= 'redshiftusername'\n",
    "password = 'password'\n",
    "\n",
    "\n",
    "#count of nulls\n",
    "\n",
    "namecount = df['name'].isnull().sum()\n",
    "\n",
    "for i in df.columns:\n",
    "  print(i, df[i].isNull().sum())\n",
    "\n",
    "\n",
    "#drop duplicates\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "#fillna\n",
    "\n",
    "df = df.fillna({'age':30, 'salary':50000})\n",
    "\n",
    "#create a new column salary category\n",
    "\n",
    "df = df.withColumn(\n",
    "    'salarycategory',\n",
    "    when(col('salary') < 50000, 'Low').when((col('salary') >= 50000) & (col('salary') < 60000), 'Medium').otherwise('High')\n",
    ")\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "s3_target_path = 's3://bucket/data/'\n",
    "df.write.mode('overwrite').parquet(s3_target_path)\n",
    "\n",
    "#write to redshift\n",
    "\n",
    "df.write.format('jdbc').option('url',redshift_url).option('dbtable', redshift_table).option('user', username).option('password', password).mode('overwrite').save()\n",
    "\n",
    "print(' the data was succesffull saved ')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ad13d4-409f-4e88-b661-85d22094ab16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "G1gB_Pw3mvvk"
   },
   "outputs": [],
   "source": [
    "#read a file from a s3 bucket, Its a csv file and run some minor tranformation on it and move it into anothe s3 bucket. use boto3 library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7c8dfee-d1af-41a0-9d16-d9d7b17e1a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EnHONH8BKoeZ"
   },
   "outputs": [],
   "source": [
    "#read a file from a s3 bucket which has date in the name of the file, we only need to read the file that has today's date\n",
    "#and any other dates we need to move the file to archive folder.#after reading the file we need to run some minor tranformation and move the file to another s3 bucket.\n",
    "\n",
    "#initiate spark session\n",
    "\n",
    "#naming convention\n",
    "rawdata\\\n",
    "  logistics\\\n",
    "  customersurvey\\\n",
    "  finance\\\n",
    "    08092025\\\n",
    "    09092025\\\n",
    "      finance_09092025\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()\n",
    "\n",
    "#mention the location of the file which is s3path\n",
    "sourcebucket = 'mybucket/source'\n",
    "destinationbucket = 'mybucket/destination'\n",
    "archivebucket = 'mybucket/archive'\n",
    "\n",
    "\n",
    "#calculte today'date\n",
    "today = datetime.now().strftime('%d%m%Y')\n",
    "\n",
    "#initiate boto3\n",
    "\n",
    "s3client = boto3.client('s3')\n",
    "\n",
    "#get all the files in my s3 bucket\n",
    "\n",
    "\n",
    "response = s3client.list_objects_v2(bucket= sourcebucket)\n",
    "\n",
    "filelist = {'data_01012025':1,'data_02012025':1}\n",
    "\n",
    "#check if there are duplicate files in my s3 bucket if its there then delete the second file.\n",
    "for i in response['Contents']:\n",
    "  key = i['Key']\n",
    "  if key in filelist:\n",
    "    s3client.delete_object(Bucket=sourcebucket, Key=key)\n",
    "    print(f'File {key} moved to archive bucket.'\n",
    "\n",
    "\n",
    "#read the file after duplicate files are deleted and then if the today date is in the key name then read and tranform and\n",
    "#load to destination or copy to archive and delete at source.\n",
    "\n",
    "for obj in response['Contents']:\n",
    "  key =obj['Key']\n",
    "\n",
    "  #look for today's date in the filename\n",
    "\n",
    "  if today in key:\n",
    "\n",
    "    #if today's date in file name read the file.\n",
    "\n",
    "    df=spark.read.csv('path', header=True, inferSchema= True)\n",
    "\n",
    "    #drop duplicates\n",
    "\n",
    "    newdf =df.withColumn('city', lit('bangalore'))\n",
    "\n",
    "\n",
    "    #export the df to anotehr bucket.\n",
    "    newdf.write.parquet(destinationbucket)\n",
    "\n",
    "    print('The file has been succesfully tranformed and moved')\n",
    "\n",
    "  else:\n",
    "\n",
    "    s3client.copy_object(CopySource={'Bucket': sourcebucket, 'Key': key}, Bucket=archivebucket, Key=key)\n",
    "    s3client.delete_object(Bucket=sourcebucket, Key=key)\n",
    "    print(f'File {key} moved to archive bucket.')\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3c4e26-7357-404d-9dbd-777de8f6eb89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ouOzFe_V6YpK"
   },
   "outputs": [],
   "source": [
    "#there are implications of cost.\n",
    "\n",
    "# lets make it faster\n",
    "# increase my cores to 4\n",
    "# num of executors to 5\n",
    "# total parallel tasks  = 20\n",
    "\n",
    "#Start applying optimisation techiques\n",
    "#broadcast join (for joins)(small df)\n",
    "#cache (large dataframe)\n",
    "#repartition > increase the number of parititions (but it shuffles the data)\n",
    "#coalesce > after tranformation we ll reduce the number of parititions.\n",
    "\n",
    "#Activate AQE : adaptive query execution : adapt to run time conditions and partition pruning , join selection.\n",
    "#catalyst optimiser : predicate pushdown (where clause), column pruning (select only the columns that we need), join reordering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cbccb59-35a0-4686-8e42-44b64a5cb6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6nj4w1xHMvl9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b32d700c-52a0-4a41-9d1f-00f893a2de47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CTbzj4384KFW"
   },
   "outputs": [],
   "source": [
    "#driver: maintains metadata nd scheduling tasks\n",
    "#executor: worker proces that run task and holds dat in memory\n",
    "#cores per executor: number of tasks each executor can run in parallel\n",
    "#memory per executor: ram avaiable for tasks and caching.\n",
    "#overhead memory: 20-30%\n",
    "\n",
    "#executor memory: (jvmheap + spark storage + shuffle + execution)\n",
    "\n",
    "#75-80% of you memory is going to heap, the rest is overhead.\n",
    "\n",
    "#rule of thumb:\n",
    "\n",
    "default memory partition size: 128mb\n",
    "total partitions needed = datasize / parition size\n",
    "\n",
    "5gb data\n",
    "5*1024 = 5120\n",
    "paritions = 5120 / 128 = 40 paritions\n",
    "\n",
    "1 parition = 1 task = 1 core\n",
    "\n",
    "4 cores in 1 executor\n",
    "for 40 paritions > 10 executors\n",
    "\n",
    "#for 1gb\n",
    "number of partitions: 1*1024/128: 8\n",
    "\n",
    "4 cores per executor : 2 executors.\n",
    "\n",
    "\n",
    "we need atleast 40 tasks to process the data efficiently\n",
    "if you have under 5 paritions tasks become heavy.\n",
    "if you over paritions 1000 for eg. scheduling process will take alot time.\n",
    "\n",
    "\n",
    "executor memroy sizing (memory + cores)\n",
    "\n",
    "executor cores: 4 (good balance)\n",
    "executor memory: 1core = approx : 2-3gb\n",
    "so 4 cores : 4*2 = 8gb - 12gb per executor\n",
    "\n",
    "\n",
    "we need 40 partitions each core can run 1 partition at a time.\n",
    "\n",
    "with 4 cores per executor : each executor can handle 4 partitions in parallel\n",
    "so 40/4 = 10 executors\n",
    "\n",
    "driver memory:\n",
    "driver doesnt do any heavy jobs its primarily coordination\n",
    "\n",
    "metadata\n",
    "shuffles\n",
    "DAG\n",
    "for 5gb input data we need approximately  4-8 gb (sufficient)\n",
    "\n",
    "\n",
    "\n",
    "#final config:\n",
    "\n",
    "data = 5g\n",
    "paritions =  40 partitions\n",
    "with 10 executors * 4 cores = 40 tasks at once.\n",
    "each executor has around 8-12 gg per executor\n",
    "\n",
    "\n",
    "#adjustments:\n",
    "if we are doing alot of joins, wide tranformations we need to increase partitions (2-3x): 100-150partitions\n",
    "if we are doing only filters and narrow transformations then 40-50 is good engough\n",
    "\n",
    "check spark ui and make changers accordingly.\n",
    "\n",
    "if there is one paritions that is taking too long to process what do we do ?\n",
    "\n",
    "1) reparitions\n",
    "2) salting (Data skewness)\n",
    "\n",
    "spark-submit\n",
    "\n",
    "-- num-executors 10\\\n",
    "--executors-cores 4\\\n",
    "--executor-memory 10g\\\n",
    "--driver-memory 6gb\n",
    "\n",
    "myfile.py\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be28e756-b347-46b7-92e2-e942f5a8c3dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "K0bvt8YTSJtU"
   },
   "outputs": [],
   "source": [
    "#20 node spark cluster\n",
    "#each node is of size - 16 cpu cores/ approx 4gb per core/ 64gb ram\n",
    "\n",
    "#each node has 3 executors : 3*4 : 12 cores per node/ 48gb ram\n",
    "\n",
    "20nodes * 3 executors : 60 executors\n",
    "\n",
    "total cpu power: 60*4: 240 cpu cores\n",
    "total memory: 60executors *4cores*4gb percore: 960 GB\n",
    "\n",
    "how many parallel tasks can we run\n",
    "\n",
    "240 cpu cores : 240 parallel tasks\n",
    "\n",
    "#10gb data from s3 buck, filtering data and how many task to run ?\n",
    "\n",
    "if we create a df for 10gb file : 80 paritions\n",
    "\n",
    "we only 20 cores\n",
    "10 secs to process 128mb data.\n",
    "first 20 tasks  run in parallel\n",
    "once these 20 tasks are done then next 20 are executed.\n",
    "\n",
    "how many cycle does it need to run\n",
    "it needs 4 cycles.\n",
    "\n",
    "#is there an possibilty of out of memory issues?\n",
    "\n",
    "each executor has 4 cores and 16gb\n",
    "\n",
    "300mb is reserved\n",
    "40% is used to store variable and data\n",
    "60 % is uised by spark memory. storage and execution memory : 50:50\n",
    "\n",
    "30% as execution memory in the memory allocated\n",
    "\n",
    "6gb memoryis what we have for execution.\n",
    "\n",
    "6gb/4: 1.25gb we actualy have to process the data.\n",
    "\n",
    "parition size  = 128mb\n",
    "\n",
    "so we have sufficient memory to run 128 mb for each parition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d9029f1-4dfc-4ffa-8b27-bfebbaca4814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nw7eoEi4QziR"
   },
   "outputs": [],
   "source": [
    "#architecture (Spark) , optimisation techniques, what is reparition, what is coalesce , when do we use them , why do we use them, same with cache, persist with examples\n",
    "#broadcast join, sortmerge join, when to use them, how does it work. Explain data skewness and what causes this and how can it resolved. what are udf's and how does it work ?\n",
    "#how to write sql statement in pyspark. how to create manual schema in spark. what was the config for mmeory allocatino for 5gb in spark.\n",
    "\n",
    "#code: extract data from one source, s3 bucket. do some simple tranformations and move data into another bucket in parquet format.\n",
    "#second highest salary using window functions in pyspark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f23a0de2-bbfd-43b9-a553-138a512754b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JrA2HBQ0GB5J"
   },
   "outputs": [],
   "source": [
    "#1) one key(partition) in a join operation has million of records while others have few, causing stage failure or long running tasks. how do we identify and handle this in pyspark?\n",
    "df.groupbykey('key').count().orderBy(desc('count')) > this will give me the count by key.\n",
    "\n",
    "1 : 1000000\n",
    "2 : 15000\n",
    "3 : 20000\n",
    "\n",
    "Solution:\n",
    "repartition\n",
    "salting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d78163e2-494f-4940-9a28-eee936487380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rwdfkgp7G-wt"
   },
   "outputs": [],
   "source": [
    "#2) you are reading 100k small files (10kb each) from s3 and the job runs very slow , how to fix this issue.\n",
    "\n",
    "#merge the files into a single and then partition the data: repartition.\n",
    "# read it into dynamicframes in glue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f27004d-7bd3-406b-998d-4caf67a4a3ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "P0jFKc-hHYX0"
   },
   "outputs": [],
   "source": [
    "#3) we have a dataset thyat is growing slowly everyday, but only need to process the data when it arrvies.\n",
    "#what is this and how to solve it.\n",
    "\n",
    "#incremental Load:\n",
    "#job bookmarks\n",
    "#DMS CDC\n",
    "#timestamping\n",
    "#versioning\n",
    "#folder structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4246b4d-220c-4701-9089-851977b8ad05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Wnob4J0lHw2D"
   },
   "outputs": [],
   "source": [
    "#4 we are joining a large fact table with a small dimension table. what is the best way to join these?\n",
    "\n",
    "#solution: Broadcast join.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378d9bf1-8c94-4b76-9be3-f3e63e154bb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rmzsySDOIDo7"
   },
   "outputs": [],
   "source": [
    "#5)  we are getting alot of data from the client into s3 and in these\n",
    "#files the client is dropping duplicate files as well by mistake how can we make sure we are not considering the duplicates files.\n",
    "\n",
    "#having appropriate folder structure.\n",
    "#verisioning: creates a new version\n",
    "boto3.list_objects_v2(bucket=sourcebucket)\n",
    "only pick the latest version files using timestamp\n",
    "#Activate job bookmarks\n",
    "\n",
    "#Avoid it at source\n",
    "#maintain a hashtable deduplication table (mysql/ postgressql)\n",
    "Create table in mysql\n",
    "fileid | filename | arrivaldate| hashcode\n",
    "\n",
    "mydata_01012025.csv\n",
    "\n",
    "list_objects_v2(bucket=sourcebucket)\n",
    "for i in reponse:\n",
    "\n",
    "\n",
    "\n",
    "#track the files that have arrived and move the duplicates into a duplicate folder using s3event and lambda.\n",
    "#boto3(s3).copyobject(duplicatefolder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89142b8e-c8a8-45af-ae5e-ffcde3883ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "oSkVfFjCJx6x"
   },
   "outputs": [],
   "source": [
    "#i have a spark job running and it keeps coming with outofmemory issues. how to solve this problem.\n",
    "\n",
    "#solution\n",
    "#reparition\n",
    "#cache and persist\n",
    "#allocate more memory\n",
    "#increase the executor memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "752e956d-3c06-422f-af56-2208d96cbaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4OKWK_UyLRZa"
   },
   "outputs": [],
   "source": [
    "#second highest salary using pyspark code using window functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36da0ec1-5973-4298-9bf5-8a9c5eefeee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "f8AY-w3iLccW"
   },
   "outputs": [],
   "source": [
    "#we have processed the data and we need to put the files into csv. the file size is huge (5GB). this will cause issues as the file size is too big for csv.\n",
    "\n",
    "#reparition before write and maxrecordsperfile this will write multiple files into s3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c687de0a-689e-4879-b960-578bd76826dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1lGyeP-1L4Wo"
   },
   "outputs": [],
   "source": [
    "#how will you execute scd type 2 in spark\n",
    "\n",
    "using withcolumn: startdate, enddate and flag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c0ac521-909d-41e1-83fc-5e4214ac38c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wZgRcNVVMAcP"
   },
   "outputs": [],
   "source": [
    "#what are the different data validation you do in your project and where did you write this code?\n",
    "#dupe check\n",
    "#row count validation\n",
    "#schema validation\n",
    "#data type validation\n",
    "#data completeness validation.\n",
    "\n",
    "#pre tranformation validation\n",
    "\n",
    "#post tranformation validation: is to check if we have carried all the tranformations as per requirements.\n",
    "\n",
    "#Athena : adhoc requirements on raw data on glue data catalog we only use when we need like a adhoc check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4df75e2-26a3-486a-aacf-ce2c158df88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CwruxwYyOSFI"
   },
   "outputs": [],
   "source": [
    "#we ahve a folder and the customer is dumping both csv and json files in my folder how do i process them together?\n",
    "\n",
    "#folder structure.\n",
    "#use unionByName() after confirming the format of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f28b1b-2224-41a5-9183-e1d33e717181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EzOMrutOOoQc"
   },
   "outputs": [],
   "source": [
    "# we have large data 500gb 2 files coming into your s3 bucket.\n",
    "#how do we optimise this ?\n",
    "\n",
    "# sortmergejoin\n",
    "# bucketing pre distribute the data based on join key.\n",
    "\n",
    "d1.write.bucketby(1000, customerid).sortby(custgomerid).saveasTable('factsales')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d703f60b-1fd0-486b-95c4-a6bdf9cc2726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Ts9Faktv8Wab"
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Pyspark1904",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
